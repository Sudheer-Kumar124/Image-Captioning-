{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b47eee8-b7ac-4d27-bbfa-8873c06f6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ry981\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ry981\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    comput tomographi scan axial view show obliter...\n",
      "1    bacteri contamin occur complet root canal trea...\n",
      "2    patient residu paralysi hand poliomyel necessa...\n",
      "3                       panoram radiograph immedi load\n",
      "4    plain abdomen xray multipl air level midabdome...\n",
      "Name: caption, dtype: object\n",
      "(65450, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure stopwords and wordnet are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # 1. Lower Case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove Links\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # 3. Remove New Lines (\\n)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # 4. Remove Words Containing Numbers\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # 5. Remove Extra Spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 6. Remove Special Characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 7. Remove Stop Words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # 8. Stemming\n",
    "    ps = PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    \n",
    "    # 9. Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# File paths\n",
    "train_path = r'D:\\archive\\all_data\\train\\radiologytraindata.csv'\n",
    "val_path = r'D:\\archive\\all_data\\validation\\radiologyvaldata.csv'\n",
    "test_path = r'D:\\archive\\all_data\\test\\radiologytestdata.csv'\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv(train_path)\n",
    "val_data = pd.read_csv(val_path)\n",
    "#test_data = pd.read_csv(test_path)\n",
    "\n",
    "# Combine all data for preprocessing\n",
    "combined_data = pd.concat([train_data])  # val_data, test_data\n",
    "\n",
    "# Apply preprocessing to the caption column\n",
    "combined_data['caption'] = combined_data['caption'].apply(preprocess_text)\n",
    "\n",
    "# Check the result\n",
    "print(combined_data['caption'].head())\n",
    "\n",
    "# Split data back into train, validation, and test sets\n",
    "train_data = combined_data.iloc[:len(train_data)]\n",
    "#val_data = combined_data.iloc[len(train_data):len(train_data) + len(val_data)]\n",
    "#test_data = combined_data.iloc[len(train_data) + len(val_data):]\n",
    "\n",
    "print(train_data.shape)\n",
    "#print(val_data.shape)\n",
    "#print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63aa69b3-bd11-408f-b958-7edece836497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                    name  \\\n",
      "0  ROCO_00002          PMC4083729_AMHSR-4-14-g002.jpg   \n",
      "1  ROCO_00003       PMC2837471_IJD2009-150251.001.jpg   \n",
      "2  ROCO_00004  PMC2505281_11999_2007_30_Fig6_HTML.jpg   \n",
      "3  ROCO_00005       PMC3745845_IJD2013-683423.005.jpg   \n",
      "4  ROCO_00007   PMC4917066_amjcaserep-17-301-g001.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0   Computed tomography scan in axial view showin...  \n",
      "1   Bacterial contamination occurred after comple...  \n",
      "2   The patient had residual paralysis of the han...  \n",
      "3    Panoramic radiograph after immediate loading.\\n  \n",
      "4   Plain abdomen x-ray: Multiple air levels at t...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the CSV file\n",
    "train_path = r'D:\\archive\\all_data\\train\\radiologytraindata.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "train_data = pd.read_csv(train_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify it was loaded correctly\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a831bab8-e7fb-440f-8e28-ade688b31f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b358672-f8ee-4795-bc01-3bf96379c145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\anaconda\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: opencv-python-headless in d:\\anaconda\\lib\\site-packages (4.10.0.82)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in d:\\anaconda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in d:\\anaconda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\anaconda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in d:\\anaconda\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Original images shape: (8176, 224, 224, 3)\n",
      "Original labels shape: (8176,)\n",
      "Augmented images shape: (32, 224, 224, 3)\n",
      "Augmented labels shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install tensorflow opencv-python-headless\n",
    "\n",
    "# Correct import statement\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the path to the extracted folder\n",
    "img_dir = r'D:\\archive\\all_data\\test\\radiology\\images'\n",
    "\n",
    "# Function to load images\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_images(img_dir, img_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img_name in os.listdir(img_dir):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "        # Assuming label is part of the filename before an underscore\n",
    "        labels.append(img_name.split('_')[0])\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "# Load the images\n",
    "images, labels = load_images(img_dir)\n",
    "\n",
    "# Normalize the images\n",
    "images = images / 255.0\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Original images shape:\", images.shape)\n",
    "print(\"Original labels shape:\", labels.shape)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the datagen on the images\n",
    "datagen.fit(images)\n",
    "\n",
    "# Example of how to use the datagen\n",
    "augmented_images, augmented_labels = next(datagen.flow(images, labels, batch_size=32))\n",
    "\n",
    "# Print shapes to verify augmentation\n",
    "print(\"Augmented images shape:\", augmented_images.shape)\n",
    "print(\"Augmented labels shape:\", augmented_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa25dc-b7d6-473b-b9cf-ea0726d1eecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
