{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing  necessary libraries"
      ],
      "metadata": {
        "id": "7DpR9drEPraw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVkFAKgZJO85"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, \\\n",
        "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading  Dataset\n"
      ],
      "metadata": {
        "id": "eM1ajRiOP1UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the Flickr8k dataset\n",
        "dataset_text = r\"/content/captions.txt\"\n",
        "dataset_images = r\"C:\\Users\\k.s.sarath chandu\\Downloads\\Flickr_8k_Dataset\\Images\"\n",
        "\n",
        "# Load the Flickr8k token file containing image filenames and captionstoken_file = os.path.join(dataset_text, 'captions.txt')\n",
        "token_file = dataset_text"
      ],
      "metadata": {
        "id": "zOTkb_I3JtqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing  Captions"
      ],
      "metadata": {
        "id": "fupI6BpOQEic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions(filename):\n",
        "    # Load captions into a dictionary\n",
        "    captions = {}\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            tokens = line.split()\n",
        "            image_id, image_caption = tokens[0], tokens[1:]\n",
        "            image_id = image_id.split('.')[0]  # Remove the image file extension\n",
        "            image_caption = ' '.join(image_caption)\n",
        "            if image_id not in captions:\n",
        "                captions[image_id] = []\n",
        "            captions[image_id].append(image_caption)\n",
        "    return captions\n",
        "\n",
        "captions = load_captions(token_file)"
      ],
      "metadata": {
        "id": "XIpW-NGrKUXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Captions\n",
        " #(load captions from the captions.txt file and\n",
        " #clean them by performing tasks like removing punctuation,\n",
        " #converting to lowercase, and removing non-alphabetic tokens.)"
      ],
      "metadata": {
        "id": "fO_OSJneQKcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_captions(captions):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for key, caption_list in captions.items():\n",
        "        for i in range(len(caption_list)):\n",
        "            caption = caption_list[i]\n",
        "            caption = caption.split()\n",
        "            caption = [word.lower() for word in caption]\n",
        "            caption = [word.translate(table) for word in caption]\n",
        "            caption = [word for word in caption if len(word)>1]\n",
        "            caption = [word for word in caption if word.isalpha()]\n",
        "            caption_list[i] =  ' '.join(caption)\n",
        "\n",
        "clean_captions(captions)"
      ],
      "metadata": {
        "id": "UTY-2654LCH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Vocabulary\n",
        "# extract all unique words from cleaned captions to create a vocabulary"
      ],
      "metadata": {
        "id": "PYzDoeMwQPWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_vocabulary(captions):\n",
        "    all_captions = []\n",
        "    for key in captions.keys():\n",
        "        [all_captions.append(d) for d in captions[key]]\n",
        "    vocabulary = set()\n",
        "    for caption in all_captions:\n",
        "        vocabulary.update(caption.split())\n",
        "    return vocabulary\n",
        "\n",
        "vocabulary = to_vocabulary(captions)"
      ],
      "metadata": {
        "id": "nFZQFODSLGqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving Cleaned Captions and Vocabulary\n",
        "#Save the cleaned captions and vocabulary to text files for future reference."
      ],
      "metadata": {
        "id": "sjc_yWxdQWvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save captions to file\n",
        "with open('captions.txt', 'w') as captions_file:\n",
        "    for key, caption_list in captions.items():\n",
        "        for caption in caption_list:\n",
        "            captions_file.write(key + '\\t' + caption + '\\n')\n",
        "\n",
        "# Save vocabulary to file\n",
        "with open('vocabulary.txt', 'w') as vocab_file:\n",
        "    vocab_file.write(\"\\n\".join(vocabulary))"
      ],
      "metadata": {
        "id": "zy_7tnH6LL_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing Images (using pre-trained InceptionV3)\n",
        "# We use the InceptionV3 model pretrained on ImageNet to extract features from images.\n",
        "# Images are resized to 299x299 pixels, preprocessed according to InceptionV3 requirements,\n",
        "# and encoded into a vector of size 2048."
      ],
      "metadata": {
        "id": "rFLYDafrQbfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = glob.glob(os.path.join(dataset_images, '*.jpg'))\n",
        "\n",
        "# Initialize dictionary to store image encodings\n",
        "encoding_train = {}\n",
        "\n",
        "# Loop over each image path and encode it\n",
        "for img_path in train_images:\n",
        "    img_name = os.path.basename(img_path)\n",
        "    encoding_train[img_name] = encode(img_path)\n",
        "\n",
        "# Function to preprocess an image using InceptionV3\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "# Function to encode an image using InceptionV3 and return its features\n",
        "def encode(img_path):\n",
        "    img = preprocess_image(img_path)\n",
        "    fea_vec = model_new.predict(img)\n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
        "    return fea_vec\n",
        "\n",
        "# Example usage:\n",
        "start = time()\n",
        "encoding_train = {}\n",
        "for img_path in train_images:\n",
        "    img_name = os.path.basename(img_path)\n",
        "    encoding_train[img_name] = encode(img_path)\n",
        "print(\"Time taken in seconds =\", time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm_C2S3RPkBB",
        "outputId": "93d32f74-c205-4265-e1bb-a054697834d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken in seconds = 0.0002663135528564453\n"
          ]
        }
      ]
    }
  ]
}