{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/Mauville/MedCLIP/blob/main/main.ipynb","timestamp":1718514224660}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"d4vQXRqNwXjf"},"source":["Imports"]},{"cell_type":"code","metadata":{"id":"ZSvBQoszjYJS"},"source":["!pip install timm\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvXzfQqgi6lT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8dd5cb6d-f681-4563-e90f-d14d7062b26f"},"source":["from IPython import display as ipythondisplay\n","from torch import nn\n","from tqdm.autonotebook import tqdm\n","from transformers import AutoTokenizer, AutoModel\n","from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n","import albumentations as A\n","import cv2\n","import gc\n","import itertools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import time\n","import timm\n","import torch\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  \n"]}]},{"cell_type":"markdown","metadata":{"id":"aF8XzKlSi6lT"},"source":["## Some pre-preocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ieBwPS2Qi6lT","outputId":"b70c6245-4d23-46c7-a328-f1d7fbecaa51"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","df = pd.read_csv('/content/drive/Shareddrives/DeepLearning/datav2/dataset.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"cA9TJM4Z8Bt-","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"9b11d214-96c1-417a-99fc-040f5859619f"},"source":["\n","# Ops to consolidate dataset to the notebook standards\n","\n","# Create a compound column using other columns. Comment lines for less or more complexity\n","\n","# \"a sagital plane\n","# MR scan\n","# of cranial trauma\n","# on a 58 year old male\n","# diagnosed with brain death\n","df[\"grouped_diag\"] = df.apply(lambda row: \\\n","                              \"a \" + str(row.Plane) + \" plane \"\\\n","                              + str(row.Core_Modality) + \" scan\"\\\n","                              + \" of \"  + str(row.Location) + \" \" + str(row.Category)\\\n","                              + \" on a \" + str(row.Patient_Age) + \" year old \" + str(row.Patient_Gender)\n","                              + \" depicting \"+ str(row.Case_Diagnosis)\n","                              , axis = 1)\n","\n","\n","#  reformat to fit the shape\n","#  \timage \tcaption id\n","df.rename(columns={\"filename\":\"image\", \"grouped_diag\":\"caption\"},inplace=True)\n","needed_cols = [\"image\",\"caption\"]\n","df = df[df.columns.intersection(needed_cols)]\n","\n","image_path = \"/content/drive/Shareddrives/DeepLearning/datav2/output\"\n","captions_path = \"/content/drive/Shareddrives/DeepLearning/datav2\"\n","\n","#  \timage \tcaption\n","# 0 \t1000268201_693b08cb0e.jpg \tA child in a pink dress is climbing up a set o...\n","# 1 \t1000268201_693b08cb0e.jpg \tA girl going into a wooden building .\n","# 2 \t1000268201_693b08cb0e.jpg \tA little girl climbing into a wooden playhouse .\n","# 3 \t1000268201_693b08cb0e.jpg \tA little girl climbing the stairs to her playh...\n","# 4 \t1000268201_693b08cb0e.jpg \tA little girl in a pink dress going into a woo...\n","\n","# I have no idea why tf he writes the dataset to memory but whatev\n","df.to_csv(captions_path + \"/captions.csv\", index=False)\n","\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>caption</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>synpic100377.jpg</td>\n","      <td>a Multiple or Montage plane CT scan of Musculo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>synpic100378.jpg</td>\n","      <td>a Multiple or Montage plane CT scan of Musculo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>synpic100379.jpg</td>\n","      <td>a Multiple or Montage plane CT scan of Musculo...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>synpic100380.jpg</td>\n","      <td>a Oblique plane XR scan of Musculoskeletal Tra...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>synpic100381.jpg</td>\n","      <td>a Multiple or Montage plane MR scan of Musculo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              image                                            caption\n","0  synpic100377.jpg  a Multiple or Montage plane CT scan of Musculo...\n","1  synpic100378.jpg  a Multiple or Montage plane CT scan of Musculo...\n","2  synpic100379.jpg  a Multiple or Montage plane CT scan of Musculo...\n","3  synpic100380.jpg  a Oblique plane XR scan of Musculoskeletal Tra...\n","4  synpic100381.jpg  a Multiple or Montage plane MR scan of Musculo..."]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"ib2zXV4r8Ayq"},"source":[]},{"cell_type":"markdown","metadata":{"id":"o54bL4gji6lU"},"source":["## Config\n","Global config for all the project."]},{"cell_type":"code","metadata":{"id":"9ulGHg9ai6lV"},"source":["class CFG:\n","    debug = False\n","    image_path = image_path\n","    captions_path = captions_path\n","    batch_size = 12\n","    num_workers = 2\n","    head_lr = 1e-3\n","    image_encoder_lr = 1e-4\n","    text_encoder_lr = 1e-5\n","    weight_decay = 1e-3\n","    patience = 1\n","    factor = 0.8\n","    epochs = 2\n","    saved_model_clinical = '/content/drive/Shareddrives/DeepLearning/datav2/withDiagnostics2.pt'\n","    trained_model = 'clinical_bert_weights.pt'\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model_name = 'resnet50'\n","    image_embedding = 2048\n","    text_encoder_model = \"distilbert-base-uncased\"\n","    clinical_encoder_model = \"emilyalsentzer/Bio_ClinicalBERT\"\n","    text_embedding = 768\n","    text_tokenizer = \"distilbert-base-uncased\"\n","    max_length = 200\n","\n","    pretrained = True # for both image encoder and text encoder\n","    trainable = True # for both image encoder and text encoder\n","    temperature = 1.0\n","\n","    # image size\n","    size = 224\n","\n","    # for projection head; used for both image and text encoders\n","    num_projection_layers = 1\n","    projection_dim = 256\n","    dropout = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"na2JGinrvT9j"},"source":["Create globally available test and validation datasets."]},{"cell_type":"code","metadata":{"id":"sedGLtLO8Ku-"},"source":["from sklearn.model_selection import train_test_split\n","\n","def make_train_valid_dfs():\n","    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n","    train, test = train_test_split(dataframe, test_size=.1, train_size=.9, shuffle=True, random_state=77,stratify=None)\n","    return train, test\n","\n","testing_df , training_df = make_train_valid_dfs()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P8OQpI9Xi6lV"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"_piLd4dxi6lV"},"source":["class AvgMeter:\n","    def __init__(self, name=\"Metric\"):\n","        self.name = name\n","        self.reset()\n","\n","    def reset(self):\n","        self.avg, self.sum, self.count = [0] * 3\n","\n","    def update(self, val, count=1):\n","        self.count += count\n","        self.sum += val * count\n","        self.avg = self.sum / self.count\n","\n","    def __repr__(self):\n","        text = f\"{self.name}: {self.avg:.4f}\"\n","        return text\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group[\"lr\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uM65_Loji6lW"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"l9V91XcNi6lW"},"source":["# Custom dataset object. Will tokenize text and apply transforms to images before yielding them.\n","\n","class CLIPDataset(torch.utils.data.Dataset):\n","    def __init__(self, image_filenames, captions, tokenizer, transforms):\n","        \"\"\"\n","        image_filenames and cpations must have the same length; so, if there are\n","        multiple captions for each image, the image_filenames must have repetitive\n","        file names\n","        \"\"\"\n","\n","        self.image_filenames = image_filenames\n","        self.captions = list(captions)\n","        self.skippedImgCount = 0\n","        self.encoded_captions = tokenizer(\n","            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n","        )\n","        self.transforms = transforms\n","\n","    def __getitem__(self, idx):\n","        item = {\n","            key: torch.tensor(values[idx])\n","            for key, values in self.encoded_captions.items()\n","        }\n","        ################################\n","        # MASSIVE GDRIVE BUG HERE\n","        # Sometimes, reading an image from disk fails, which crashes the entirety of the program\n","        # Here we default to adding the image at dataset[0]\n","        ################################\n","        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n","        if image is None:\n","          self.skippedImgCount += 1\n","          return self.__getitem__(1)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = self.transforms(image=image)['image']\n","        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n","        item['caption'] = self.captions[idx]\n","\n","        return item\n","\n","\n","    def __len__(self):\n","        return len(self.captions)\n","\n","\n","\n","def get_transforms(mode=\"train\"):\n","    if mode == \"train\":\n","        return A.Compose(\n","            [\n","                A.Resize(CFG.size, CFG.size, always_apply=True),\n","                A.Normalize(max_pixel_value=255.0, always_apply=True),\n","            ]\n","        )\n","    else:\n","        return A.Compose(\n","            [\n","                A.Resize(CFG.size, CFG.size, always_apply=True),\n","                A.Normalize(max_pixel_value=255.0, always_apply=True),\n","            ]\n","        )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yk394aMmi6lX"},"source":["## Image Encoder"]},{"cell_type":"code","metadata":{"id":"i0flfMTRi6lY"},"source":["class ImageEncoder(nn.Module):\n","    \"\"\"\n","    Encode images to a fixed size vector\n","    \"\"\"\n","\n","    def __init__(\n","        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n","    ):\n","        super().__init__()\n","        self.model = timm.create_model(\n","            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n","        )\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","    def forward(self, x):\n","        return self.model(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JKRoQ0o9i6lY"},"source":["## Text Encoder"]},{"cell_type":"code","metadata":{"id":"am5VR4Ezi6lZ"},"source":["class TextEncoder(nn.Module):\n","    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n","        super().__init__()\n","        if pretrained:\n","            # self.model = DistilBertModel.from_pretrained(model_name)\n","\n","            # Use Bio-ClinicalBERT\n","            self.model = AutoModel.from_pretrained(CFG.clinical_encoder_model)\n","\n","        else:\n","            self.model = DistilBertModel(config=DistilBertConfig())\n","\n","        for p in self.model.parameters():\n","            p.requires_grad = trainable\n","\n","        # we are using the CLS token hidden representation as the sentence's embedding\n","        self.target_token_idx = 0\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden_state = output.last_hidden_state\n","        return last_hidden_state[:, self.target_token_idx, :]"],"execution_count":null,"outputs":[]}]}