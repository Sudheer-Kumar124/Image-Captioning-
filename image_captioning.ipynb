{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i0RcpIUlwhyI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications import efficientnet\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "keras.utils.set_random_seed(111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "38IdINQIwhyK"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -qq Flickr8k_Dataset.zip\n",
        "!unzip -qq Flickr8k_text.zip\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "64PNmgyowhyK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Path to the images\n",
        "IMAGES_PATH = \"Flicker8k_Dataset\"\n",
        "\n",
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBjCK-CZwhyL"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FBnJ00tqwhyL",
        "outputId": "18527901-7df0-4367-e27e-20e296c099be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  6114\n",
            "Number of validation samples:  1529\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_captions_data(filename):\n",
        "    \"\"\"Loads captions (text) data and maps them to corresponding images.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to the text file containing caption data.\n",
        "\n",
        "    Returns:\n",
        "        caption_mapping: Dictionary mapping image names and the corresponding captions\n",
        "        text_data: List containing all the available captions\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename) as caption_file:\n",
        "        caption_data = caption_file.readlines()\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "        images_to_skip = set()\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            # Image name and captions are separated using a tab\n",
        "            img_name, caption = line.split(\"\\t\")\n",
        "\n",
        "            # Each image is repeated five times for the five different captions.\n",
        "            # Each image name has a suffix `#(caption_number)`\n",
        "            img_name = img_name.split(\"#\")[0]\n",
        "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "\n",
        "            # We will remove caption that are either too short to too long\n",
        "            tokens = caption.strip().split()\n",
        "\n",
        "            if len(tokens) < 5 or len(tokens) > SEQ_LENGTH:\n",
        "                images_to_skip.add(img_name)\n",
        "                continue\n",
        "\n",
        "            if img_name.endswith(\"jpg\") and img_name not in images_to_skip:\n",
        "                # We will add a start and an end token to each caption\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "\n",
        "                if img_name in caption_mapping:\n",
        "                    caption_mapping[img_name].append(caption)\n",
        "                else:\n",
        "                    caption_mapping[img_name] = [caption]\n",
        "\n",
        "        for img_name in images_to_skip:\n",
        "            if img_name in caption_mapping:\n",
        "                del caption_mapping[img_name]\n",
        "\n",
        "        return caption_mapping, text_data\n",
        "\n",
        "\n",
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    \"\"\"Split the captioning dataset into train and validation sets.\n",
        "\n",
        "    Args:\n",
        "        caption_data (dict): Dictionary containing the mapped caption data\n",
        "        train_size (float): Fraction of all the full dataset to use as training data\n",
        "        shuffle (bool): Whether to shuffle the dataset before splitting\n",
        "\n",
        "    Returns:\n",
        "        Traning and validation datasets as two separated dicts\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get the list of all image names\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Shuffle if necessary\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Split into training and validation sets\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    # 4. Return the splits\n",
        "    return training_data, validation_data\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, valid_data = train_val_split(captions_mapping)\n",
        "print(\"Number of training samples: \", len(train_data))\n",
        "print(\"Number of validation samples: \", len(valid_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pwm6Mfo2whyM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "vectorization.adapt(text_data)\n",
        "\n",
        "# Data augmentation for image data\n",
        "image_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomContrast(0.3),\n",
        "        layers.RandomRotation(0.3)\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KLA6jZJwhyN"
      },
      "source": [
        "## Building a `tf.data.Dataset` pipeline for training\n",
        "\n",
        "We will generate pairs of images and corresponding captions using a `tf.data.Dataset` object.\n",
        "The pipeline consists of two steps:\n",
        "\n",
        "1. Read the image from the disk\n",
        "2. Tokenize all the five captions corresponding to the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A7hAVAVwwhyO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def decode_and_resize(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    ## image resizing\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_input(img_path, captions):\n",
        "    return decode_and_resize(img_path), vectorization(captions)\n",
        "\n",
        "\n",
        "def make_dataset(images, captions):\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, captions))\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
        "    dataset = dataset.map(process_input, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pass the list of images and the list of corresponding captions\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img, cap in train_dataset.take(1):\n",
        "    print(img.shape, cap)"
      ],
      "metadata": {
        "id": "XUEL6Sj5I8HX",
        "outputId": "af24e239-3b73-412f-e0a5-2172ff103749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 299, 299, 3) tf.Tensor(\n",
            "[[[   3    2   17 ...    0    0    0]\n",
            "  [   3    2   17 ...    0    0    0]\n",
            "  [   3    2  228 ...    0    0    0]\n",
            "  [   3    2   27 ...    0    0    0]\n",
            "  [   3   14   63 ...    0    0    0]]\n",
            "\n",
            " [[   3   14   64 ...    0    0    0]\n",
            "  [   3   14   64 ...    0    0    0]\n",
            "  [   3   14   64 ...    0    0    0]\n",
            "  [   3   14   27 ...    0    0    0]\n",
            "  [   3   14   27 ...    0    0    0]]\n",
            "\n",
            " [[   3    2   12 ...    0    0    0]\n",
            "  [   3    2   12 ...    0    0    0]\n",
            "  [   3    2   44 ...    0    0    0]\n",
            "  [   3    2   44 ...    0    0    0]\n",
            "  [   3   12    7 ...    0    0    0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[   3    2   17 ...    0    0    0]\n",
            "  [   3    2   17 ...    0    0    0]\n",
            "  [   3    2   17 ...    0    0    0]\n",
            "  [   3    2   12 ...    0    0    0]\n",
            "  [   3    2 3378 ...    0    0    0]]\n",
            "\n",
            " [[   3    2   10 ...    0    0    0]\n",
            "  [   3    2   10 ...    0    0    0]\n",
            "  [   3    2 2271 ...    0    0    0]\n",
            "  [   3    6  661 ...    0    0    0]\n",
            "  [   3    6   28 ...    0    0    0]]\n",
            "\n",
            " [[   3    2   94 ...    0    0    0]\n",
            "  [   3    2   12 ...    0    0    0]\n",
            "  [   3    2   44 ...    0    0    0]\n",
            "  [   3    2  276 ...    0    0    0]\n",
            "  [   3    2  276 ...    0    0    0]]], shape=(64, 5, 25), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1kZ9ZLbI-Cu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zy_kECWgJ_L6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_captioning",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}