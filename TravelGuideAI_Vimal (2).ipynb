{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpjSp2xp3tXt"
      },
      "source": [
        "Install Kaggle Package\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gNg-mvZweVBd"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4icSdCbv31UU"
      },
      "source": [
        " Mounts Google Drive to the Colab environment, making files accessible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm_BlA1KeiKR",
        "outputId": "098e8f49-a724-4a45-ba79-1ab3155d283e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlgNRs9J3-mj"
      },
      "source": [
        "Create directory for the Kaggle API configuration file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5KMK4_FFfHqy"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /root/.kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j60GqGzR4g3b"
      },
      "source": [
        "Copies Kaggle API key (kaggle.json) from Google Drive to the .kaggle directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZLhccPvNflhw"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCPgrDf54mfr"
      },
      "source": [
        "Set Permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UR1bV2rTgPEi"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmM4A8M84tIE"
      },
      "source": [
        "Downloads the dataset from Kaggle using the dataset identifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M9Q3cBbgWD2",
        "outputId": "06a5c287-3586-4249-af4c-cded4342a54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/danushkumarv/indian-monuments-image-dataset\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading indian-monuments-image-dataset.zip to /content\n",
            " 98% 609M/621M [00:09<00:00, 78.8MB/s]\n",
            "100% 621M/621M [00:09<00:00, 65.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d danushkumarv/indian-monuments-image-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pAipDJH47jo"
      },
      "source": [
        "Extracts the downloaded ZIP file into the ('/content') directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "arLE_hyEgrs4"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('indian-monuments-image-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mThe-sPg1dP",
        "outputId": "a7d92fe0-adf9-4877-9b42-d9c0ede7704b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Khajuraho', 'tanjavur temple', 'qutub_minar', 'basilica_of_bom_jesus', 'Gateway of India', 'Chhota_Imambara', 'victoria memorial', 'tajmahal', 'jamali_kamali_tomb', 'Humayun_s Tomb', 'Fatehpur Sikri', 'charminar', 'Ellora Caves', 'iron_pillar', 'Ajanta Caves', 'golden temple', 'mysore_palace', 'Sun Temple Konark', 'alai_darwaza', 'lotus_temple', 'Charar-E- Sharif', 'India gate pics', 'alai_minar', 'hawa mahal pics']\n"
          ]
        }
      ],
      "source": [
        "#import the libraries\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Function to Load Data\n",
        "\n",
        "def get_data_from_folder(path):\n",
        "  class_names = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "\n",
        "  IMG_SIZE = 224\n",
        "  data = []\n",
        "  for class_name in class_names:\n",
        "    try:\n",
        "      files = glob.glob(os.path.join(path, class_name, \"*\"))\n",
        "      for f in files:\n",
        "        img = cv2.imread(f)\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        data.append([np.array(img), class_names.index(class_name)])\n",
        "    except:\n",
        "      pass\n",
        "  np.random.shuffle(data)\n",
        "  return data, class_names\n",
        "\n",
        "#Load Training and Testing Data\n",
        "\n",
        "training_data, training_class_names = get_data_from_folder('/content/Indian-monuments/images/train')\n",
        "testing_data, testing_class_names = get_data_from_folder('/content/Indian-monuments/images/test')\n",
        "\n",
        "print(training_class_names)  # This will print the extracted monument types for training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GUAs2fi8hXZd"
      },
      "outputs": [],
      "source": [
        "#Image Preprocessing Function\n",
        "\n",
        "import numpy as np\n",
        "def preprocess_image(img):\n",
        "\n",
        "  img = img / 255.0\n",
        "\n",
        "\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  return img\n",
        "\n",
        "#Preprocess Training and Testing Data\n",
        "\n",
        "for i, (img, label) in enumerate(training_data):\n",
        "  training_data[i][0] = preprocess_image(img)\n",
        "\n",
        "for i, (img, label) in enumerate(testing_data):\n",
        "  testing_data[i][0] = preprocess_image(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkVArXqXhhCW",
        "outputId": "3e39526b-67e1-437e-9d65-04ac6105a22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Import OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#Separate and Encode Labels\n",
        "\n",
        "training_labels = np.array([label for _, label in training_data])\n",
        "testing_labels = np.array([label for _, label in testing_data])\n",
        "\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoder.fit(training_labels.reshape(-1, 1))\n",
        "\n",
        "\n",
        "training_labels = encoder.transform(training_labels.reshape(-1, 1))\n",
        "testing_labels = encoder.transform(testing_labels.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RkLPYE_ZutZ",
        "outputId": "2e791e69-a84b-4bf4-93b7-0382296a44ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3666 images belonging to 24 classes.\n",
            "Found 1049 images belonging to 24 classes.\n",
            "Epoch 1/5\n",
            "115/115 [==============================] - 366s 3s/step - loss: 43.6087 - accuracy: 0.0747 - val_loss: 3.1300 - val_accuracy: 0.1049\n",
            "Epoch 2/5\n",
            "115/115 [==============================] - 367s 3s/step - loss: 2.9286 - accuracy: 0.1626 - val_loss: 2.8972 - val_accuracy: 0.2231\n",
            "Epoch 3/5\n",
            "115/115 [==============================] - 376s 3s/step - loss: 2.3064 - accuracy: 0.3622 - val_loss: 2.9672 - val_accuracy: 0.3108\n",
            "Epoch 4/5\n",
            "115/115 [==============================] - 366s 3s/step - loss: 1.6054 - accuracy: 0.5363 - val_loss: 3.0107 - val_accuracy: 0.3622\n",
            "Epoch 5/5\n",
            "115/115 [==============================] - 380s 3s/step - loss: 1.0859 - accuracy: 0.6836 - val_loss: 3.7222 - val_accuracy: 0.3870\n",
            "33/33 [==============================] - 31s 929ms/step - loss: 3.7222 - accuracy: 0.3870\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.7222142219543457, 0.3870352804660797]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the image size\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Define the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(training_class_names), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Data generators for efficient batch processing\n",
        "datagen = ImageDataGenerator(rescale=0.2)\n",
        "\n",
        "# Replace 'path_to_train_data' and 'path_to_validation_data' with the actual paths\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/Indian-monuments/images/train', # Update with the correct path\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    '/content/Indian-monuments/images/test', # Update with the correct path\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "cnn_model.evaluate(validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL-PuISEqO1H",
        "outputId": "e04a2c20-072a-4af5-a7be-8b4f10f9ab2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 50ms/step\n",
            "The predicted class is: mysore_palace\n"
          ]
        }
      ],
      "source": [
        "# Predictive system\n",
        "def predict_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = preprocess_image(img)\n",
        "    prediction = cnn_model.predict(img)\n",
        "    predicted_class = training_class_names[np.argmax(prediction)]\n",
        "    return predicted_class\n",
        "\n",
        "# Example usage:\n",
        "image_path = '/content/Indian-monuments/images/test/Ajanta Caves/1.jpg'  # Update with an actual image path\n",
        "predicted_class = predict_image(image_path)\n",
        "print(f'The predicted class is: {predicted_class}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}